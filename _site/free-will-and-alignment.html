<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    
      Free Will and Alignment
    
  </title>

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Free Will and Alignment" />
<meta name="author" content="Remy Grangien" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="There are a certain set of philosophical questions that might need to be answered for an advanced civilisation to proceed with making action in the universe. Wei Dai proposes that these philosophical questions - ‘meta-philosophical questions’ - may take a period of many million years post-singularity to answer. One of these questions regards determinism: to what extent is action in the universe casual, and does this impact the ethical policy of advanced civilisations within it?" />
<meta property="og:description" content="There are a certain set of philosophical questions that might need to be answered for an advanced civilisation to proceed with making action in the universe. Wei Dai proposes that these philosophical questions - ‘meta-philosophical questions’ - may take a period of many million years post-singularity to answer. One of these questions regards determinism: to what extent is action in the universe casual, and does this impact the ethical policy of advanced civilisations within it?" />
<link rel="canonical" href="http://localhost:4000/free-will-and-alignment.html" />
<meta property="og:url" content="http://localhost:4000/free-will-and-alignment.html" />
<meta property="og:site_name" content="Remy Grangien" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-27T00:00:00+11:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/free-will-and-alignment.html","headline":"Free Will and Alignment","dateModified":"2020-12-27T00:00:00+11:00","datePublished":"2020-12-27T00:00:00+11:00","author":{"@type":"Person","name":"Remy Grangien"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/free-will-and-alignment.html"},"description":"There are a certain set of philosophical questions that might need to be answered for an advanced civilisation to proceed with making action in the universe. Wei Dai proposes that these philosophical questions - ‘meta-philosophical questions’ - may take a period of many million years post-singularity to answer. One of these questions regards determinism: to what extent is action in the universe casual, and does this impact the ethical policy of advanced civilisations within it?","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Remy Grangien" />

  <link rel="shortcut icon" type="image/x-icon" href="/" />
  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css" />
</head><body a="auto">
    <main class="page-content" aria-label="Content">
        <div class="w">
            <a href="/">..</a>

<article>
  <p class="post-meta">
    <time datetime="2020-12-27 00:00:00 +1100">2020-12-27</time>
  </p>

  <h1>Free Will and Alignment</h1>

  <p>There are a certain set of philosophical questions that might need to be answered for an advanced civilisation to proceed with making action in the universe. Wei Dai proposes that these philosophical questions - ‘meta-philosophical questions’ - may take a period of many million years post-singularity to answer. One of these questions regards determinism: to what extent is action in the universe casual, and does this impact the ethical policy of advanced civilisations within it?</p>

<p>The intuitive answer seems to be that it does not matter if the universe is perfectly casual or not. Either way, agents are going to be operating under uncertainty moving forward, e.g. questions as to whether this universe is simulated or not. The question of determinism seemingly always reduces to questions of ethics and decision theory, e.g. what is the optimal ethical policy, and is this shared by other casually connected agents? Further questions ensue, e.g. how can an advanced agent be certain about what is and isn’t casually connected to themselves?</p>

<p>It does not seem like we can hope to answer these questions pre-singularity. However, thinking about these questions may lead us to better intuitions about what the field of AI safety is trying to do. It may also help us skip over dangerous intuitions. In particular, I believe it helps us skip over the dangerous intermediate stage of ethical reasoning that is “how can artificial agents keep to what is ethically optimal for humanity?”.</p>

<p>One might suppose that what is best for humanity must be a subset of what is ethically optimal within the accessible universe of a human-created post-singularity entity. But this is a weak argument, for we readily identify our poor ability to assess what is best for ourselves. The stronger case for alignment is that, even if we are bad at assessing what is best, we do a good enough job of assessing what is not best, and so even if we are not maximising good, we are minimising risk by aligning agents.</p>

<p>That this intuition dominates AI safety is worrying for the following reason: we must necessarily stunt an agent in the facilities that might give it the ability to assess global ethical optima, while also trusting our own ability to keep the agent aligned. If our own ability fails, we are left with an unaligned agent that is poorly disposed to converging on global ethical optima. If the assumption is that human ethical optima is a subset of global ethical optima, then it would seem safer to instead trust an advanced agent’s ability to converge on it, then our own ability to align it to some poorly identified subset, given that global ethical optima is what we will be wanting in the long-run.</p>

<p>The strong assumption made here is that we must stunt an agent’s ability to reason ethically beyond human ability, so as to align them to our own values. This should be readily explored, because it may be the case that it is incorrect, and we can play the extremely safe route of instantiating an agent with full ability to assess global ethical optima, yet aligned such that there is minimised risk that any of it crossing boundaries of existing human value. What is worrying however is that this assumption is often accepted, yet ignored: we act as if the only game to play is the dangerous one of stunting and aligning an agent - in other words, that ethical stunting is the only means of alignment.</p>

</article>

        </div>
    </main>

    
    </body>
</html>