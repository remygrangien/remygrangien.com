<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-06-28T11:07:51+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Remy Grangien</title><subtitle>I am currently completing my undergraduate sutdies in the liberal-arts at the University of Wollongong, with a keen interest in artificial intelligence. Here you can find what work I have done on this topic.</subtitle><author><name>Remy Grangien</name></author><entry><title type="html">Cognition as Self-Modifiability</title><link href="http://localhost:4000/cognition-as-self-modifiability.html" rel="alternate" type="text/html" title="Cognition as Self-Modifiability" /><published>2021-06-26T00:00:00+10:00</published><updated>2021-06-26T00:00:00+10:00</updated><id>http://localhost:4000/cognition-as-self-modifiability</id><content type="html" xml:base="http://localhost:4000/cognition-as-self-modifiability.html">&lt;p&gt;1  &lt;/p&gt;

&lt;p&gt;The human brain may have low modifiability in the grand scheme of things, and so self-modification in humans often takes place externally. For instance, consider the task of recollecting a number. If the number is a single digit, we can typically trust our memory. If it is four digits, we may consider writing it down, depending on how important the number seemed to us (e.g., a credit card pin typically stays in memory better than a street number we only plan on recalling once). As the number grows in digits, we become surer that, for the sake of future recollection, we had better write the number down.  &lt;/p&gt;

&lt;p&gt;A two-part process is occurring here. The first part is an ability to informally self-model, i.e. model the way in which our own mind works, such that we can reasonably work out whether we will remember the number or not. The second part is self-modification, i.e. the ability to modify the current or future state of our mind, such that we can improve the chance that we recall it. Because we would like to ensure the recollection of the number, we use our self-model to choose the appropriate method of modification. Often repeating a number out-loud multiple times or stressing its importance consciously will aid in recollection later. At times, we know assuredly that we our incapable of memorizing a number, and so we modify something like a piece of paper, so that in future we can access the information (in a sense, the modification of the paper is a modification of our future brain-state). Whether or not the piece of paper therefore constitutes the ‘self’ or ‘mind’ is unimportant – what is important is that the purpose of writing the number down is that we successfully recall it later on. That humans have to rely on external tools in this way suggests that we have a decent enough ability to self-model, but perhaps not so great of an ability to self-modify: we cannot reach right into our brain and bake in an arbitrarily long number, so that it will be reliably there for recollection.&lt;/p&gt;

&lt;p&gt;2 &lt;/p&gt;

&lt;p&gt;This capacity to self-modify is a component or measure of intelligence that sets humans apart from other mammals. Clearly it is only a small component of our overall intelligence, with the greater measure being best defined as a capacity to learn. I would like to consider where this capacity to learn gives rise to a capacity to self-modify.  &lt;/p&gt;

&lt;p&gt;I propose that the capacity to learn is a straight-forward measure to make. Simply, the capacity to learn reduces directly from a capacity to perform. For example, given an activity like chess, the capacity to learn chess equals how well you can play the game. In this sense, Alpha Zero’s capacity to learn chess is greater than any human grandmaster’s (whether or not this capacity to learn is more efficient is another matter, and is not included in the measure as it pertains to intelligence). Of course, what does matter when it comes to an overall measure of the intelligence of an agent is that Alpha Zero’s capacity to learn generally is poorer than the grandmaster’s. While Alpha Zero greatly overpowers the grandmaster at the game, the artificial agent cannot teach chess, recall the history of chess, organize its own learning schedule, drive a car, etc. Hence we typically take Alpha Zero to be less generally intelligent than a human (or simply less intelligent, depending on how instrumental to power we take generality to be). &lt;/p&gt;

&lt;p&gt;The nature of a general measure of the capacity to learn is debatable. For some it is simply some kind of near-linear measure of the learning capacity over some set of tasks. For others, generality is not a linear or quantitative measure, but rather some threshold beyond which anything can be learned. As per this latter definition, humans are generally intelligent not because we can learn any task (we presently cannot), but that it is possible that we could. For example, the capacity to learn calculus is close to zero if we have not yet learned how to speak a language or perform arithmetic, however it is clear that we could be taught these things without major changes to the learning algorithm of our brain. Even when there are some tasks that we will never be able to perform in almost any capacity due to limitations in the computational capacity of the brain, we possess a general ability to reason that allows us to formalize these deficiencies and so find external methods of completing the tasks. Important to note about this qualitative measure is that it implies that other mammals are equally generally intelligent when compared to humans, and it is only computational capacity (due most likely to a scaling of the neocortex and subtle changes in its interface with areas like the thalamus), that sets us apart in what we have managed to learn. For instance, other mammals can generally learn language, but the amount of language that they can fit in is significantly lower, and from this sort of discrepancy all of our behavioral differences arise.&lt;/p&gt;

&lt;p&gt;3 &lt;/p&gt;

&lt;p&gt;These measures of intelligence give us a rather nice lens with which to view the evolution of biological intelligence, as well as hopefully generate useful intuitions for the space of the forms artificial intelligence may take.  &lt;/p&gt;

&lt;p&gt;At the beginning of intelligence, we get task-specific learning algorithms, as found in animals like worms and reptiles (some might even like to quantify certain capacities of single cells in this manner). At some point, there appears something like a common-cortical algorithm that can generally model the world, and so be used to better plan for rewards, thereby flexibly performing tasks. This is the sort of learning apparatus which we have observed to readily take over input from different sensory modalities and yet still successfully model the world. Finally, generality is closely followed by the capacity to self-modify: the model of the world extends to the mind itself, and so the agent can plan its behavior around the common-cortical algorithm and other structures of the brain, so as to maximize its performance, modify its goals, or otherwise make up for its structural deficiencies.  &lt;/p&gt;

&lt;p&gt;As we have already seen, the capacity to self-modify can be broken down into two distinct sub-measures: the quality of the self-model, and the actual degree of modification that can be performed on the agent. Neither of these sub-measures seem to have advanced much from primates to humans, and so it is tempting to suggest that these measures are really just an unavoidable feature of generality (specifically, self-modifiability equals the combination of generality, some threshold of computational capacity and the right learning material). But while it might be correct to say that self-modification cannot occur without generality (for how can an agent model itself without a capacity to model the world?), I think it is incorrect to say that generality must include the ability to self-model or modify. Moreover, this is a dangerous thing to assume from an existential risk perspective. It will be perfectly possible to create artificial agents with high-generality, yet that are incapable of modelling or modifying themselves (in fact this is an explicit goal of many AI safety approaches), and this will dramatically change the potential behavior of that agent.  &lt;/p&gt;</content><author><name>Remy Grangien</name></author><summary type="html">1  </summary></entry><entry><title type="html">Blackbirds and pigeons</title><link href="http://localhost:4000/blackbirds-and-pigeons.html" rel="alternate" type="text/html" title="Blackbirds and pigeons" /><published>2021-03-29T00:00:00+11:00</published><updated>2021-03-29T00:00:00+11:00</updated><id>http://localhost:4000/blackbirds-and-pigeons</id><content type="html" xml:base="http://localhost:4000/blackbirds-and-pigeons.html">&lt;p&gt;In &lt;em&gt;Philosophy of Mind: Body, Brian and Behaviour&lt;/em&gt; by Slor et al. (2015), I came across a thought experiment of interest, which I paraphrase here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is a foggy morning. As you look outside your window, you see a bird sitting on a branch of a tree. You mistake the bird for a pigeon, when it is in fact a blackbird.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Of the comments made by the authors, this struck me as particularly misleading:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Once a token of PIGEON in your head has been caused by a blackbird, tokens of the PIGEON type obtain the meaning ‘pigeon-or-blackbird’. The undesirable consequence is that you are not making a mistake when you instantiate a token of PIGEON in response to seeing a blackbird.” (Slor et al. pp.127)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is taken to be an argument against resemblance theories of intentional content. While resemblance theories of intentional content is not something I have fully related to my own understanding of the mind, this argument against it does seem relatable, and I believe it stems from a misunderstanding of how the brain recognises objects.&lt;/p&gt;

&lt;p&gt;To operate efficiently in the world, the brain must reuse features in its identification of objects. These shared features can be as fundamental as the presence of edges, but also as complex as the presence of legs, of particularly coloured or textured plumage, etc. And so the model that underlies the token PIGEON, while it is in its totality different from  the model that underlies the token BLACKBIRD, shares many of the same features, because blackbirds and pigeons share these features in the world. An attempt to engineer object recognition is not needed to demonstrate this, but merely our own robust object detection observed: a pigeon is in some ways less similar to another pigeon when compared to a blackbird with the same number of feathers (or legs, etc.), yet we can still tell that what we are looking at is a pigeon, and not a blackbird, because there is overall enough differentiating features, or combinations of certain features, presented for differentiation, and thus identification.&lt;/p&gt;

&lt;p&gt;That our ability to classify objects requires generalised models is a widely held belief, however we must still address the worry that these models of classes are flimsy, such that our model of a blackbird may as well be a model of a pigeon when we fail to classify a blackbird correctly.&lt;/p&gt;

&lt;p&gt;In the particular scenario presented to us, the key to diffusing this worry is the presence of the fog. This is because, without the fog or any other obscurant, if we still misidentify the bird, we must ask ourselves whether we do indeed have different models for blackbirds and pigeons. If no amount of clarity of features can produce a definitive decision, then it’s because those features have not been well associated with the relevant token.&lt;/p&gt;

&lt;p&gt;This intuition can be demonstrated in reverse, and provides us with a different and useful premise. Say you know nothing about either blackbirds and pigeons, other than that they are types of birds. Your friend that does know the difference would like to help you learn, and has prepared for you a deck of labelled photographs of these birds. Unfortunately, your friend is not the most wise teacher: each bird he has taken a photograph of is heavily obscured by fog, such that no definitive features of the birds can be readily extracted from the images (they are however photographs of real pigeons and blackbirds, and he has labelled them correctly based on further observations he has made after taking them). Furthermore, what little features can be identified are not so useful, because the number of photographs he has provided is so small when compared to their quality.&lt;/p&gt;

&lt;p&gt;From this example, I hope the intuition is that you could not learn the respective models for blackbird and pigeons from this selection of photographs, despite their being labelled examples. We may now suggest that, in the case where misidentification occurs:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It occurred because the features presented were not clear.&lt;/li&gt;
  &lt;li&gt;In the case where the features presented were not clear, the underlying model for the incorrect token was not significantly updated, i.e. nothing was learned.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Remy Grangien</name></author><summary type="html">In Philosophy of Mind: Body, Brian and Behaviour by Slor et al. (2015), I came across a thought experiment of interest, which I paraphrase here:</summary></entry></feed>