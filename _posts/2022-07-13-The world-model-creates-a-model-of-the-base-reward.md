---
layout: post
title: "The world model creates a model of the base-reward”
---

_Context: many if not most contemporary theories of intelligence have their analog of ‘world-model’, ‘reward-model’, and ‘base-reward’, and they relate fairly similarly to what I propose. The explanation I provide is a part of an attempt to more cleanly define the three at an intuitive level, where the intuitions are arising from working backwards from human examples. Furthermore, the sort of agent foundations that follows from the orthogonality thesis typically gloss over or straight up ignore the potential necessity of a reward-model for general intelligence to work (given the kind of compute humans will reasonably have access to before AGI can begin recursive self-improvement)._

There is this core intuition that plenty of AI safety research follows: the base-reward alone prompts the world-model to predict for future reward. For instance, from a neutral state (i.e. the world-model is not processing a whole lot), a small reward signal comes in that begins rewarding thought relating to food. The small reward initially prompts merely some vague thoughts about food; this brings in more reward when particularly rewarding foods at that moment are thought about, etc. As the reward signal amplifies, the world-model passes a threshold where reward would be maximised if it actually started planning about how to get that food. Once a sufficiently easy or tenable plan is generated, some expected reward threshold is passed where the plan gets put into action by the agent. 

Infants seem to demonstrate this kind of straightforward relationship between the base-reward and world-model. They will be extremely upset about some thing they want (a particular food), yet if you say a word associated with some other rewarding activity, e.g. a show they like to watch, within milliseconds their entire action-plan flips. The infant runs from the fridge they were clawing at toward the television, anticipating the show will appear there. Here, the single word mentioned by the parent elicits the initial prediction of reward via some other dormant reward signal, enough so that upon a simple back and forth between the base-reward and world-model they expect more reward from that other signal. 

Yet infants up to the age of two cannot plan over periods much longer than a handful of minutes. While the complexity of the plan would seem to be a function of the complexity of their world-model, I do not think the limited duration of the plan fully correlates with this limited complexity. Instead, my hypothesis is that what must develop between infancy and older age is a sufficiently sophisticated model of the base-reward. By ‘sophisticated’, I mean that the reward-model will have many qualities of the world-model, such that it is capable of predicting counterfactuals of the kind: ‘what would be most rewarding in the future given that I might be craving this particular reward then, even though I am not craving it now?’. The key thing that differentiates the base-reward and reward-model is that the former is the actual instantiation of several reward functions, whereas the latter is a representation of those function, so that they can be searched over temporarily. 

If the human brain could not do this, then our effectiveness in even the ancestral environment would be heavily curtailed. The brain would tend to stop building shelter for cold nights given the body was sufficiently warm at present; the brain would tend to stop making long distance travel plans to non-pre-programmed locations for the purpose of finding food, given that it was satiated at present, etc. 

### A dispassionate reward-model is required if your base-reward is setup to avoid wire-heading

An obvious counter-argument is that, from experience, our thought about a particular reward does seem to massively drop off the moment it is satisfied. This must be the case to stop wire-heading, which seems to be one core heuristic our base-reward is constructed with. If we didn’t stop finding sex pleasurable after just having it, we would quickly die off from repeatedly engaging in the activity at the expense of all else. Yet if absolutely all prediction of future reward dropped off the moment the base-reward was satiated, we’d again be highly ineffective planners. If we were to do something immediately after the event that would irrevocably ruin the relationship, we  immediately understand the implications this would have for future reward! 

More cleanly: a sophisticated reward-model is necessary to achieve a balance between wire-heading and long-term reward maximisation for effective planning.  The base-reward for sex must vary the way it does to avoid wire-heading, yet otherwise our world-model must be put to use to plan for its regular attainment over the long-term. 

By having this more readily available reward-model to plan around, the world-model also gets an easier time communicating evaluations in such a way that is conducive to cooperation and negotiation. If the evaluation-driven behaviour a person engages in was completely separate from a descriptive world-model (that could otherwise predict generally for the state of the world), then that person would be unable to account for what it was that they were doing or just did. While it seems the case that the world-model could over time infer what evaluations must have been required to bring about its own behaviour, it would be a much more efficient and reliable process to just correlate action outputs with the base reward signal already coming into the world-model. After a while, the world-model could rely almost exclusively on the reward-model, having fit the base reward well enough that little contradicting evaluations came in based on long-term predictions. Of course, we do make terrible long-term predictions about future reward from time to time, and this would seem to be unavoidable given the shape of some of our base reward functions, and the limitations of our world-model. (Note: Scott Alexander writes about this well [here][1], and about the like-want distinction more fundamentally elsewhere, although I cannot currently find it). This is another way of saying we don’t have direct access to the specification of the base-reward function, and that anyway the world is too unpredictable to reliably know what state it will be on longer time-frames. 

Further simple evidence of the above is that we have a sophisticated reward-model of other humans around us (both generally, but also specific to individuals we know well), and obviously we don’t have direct access to other people’s base-reward. 

### Potential take-aways and validity

An agent’s need to think counter-factually about the base-reward would seem to be necessary if the base-reward was sufficiently complex, such that there was no immediately obvious way to maximise reward over long periods of time. The world-model would need to train up a representation of the actual shape of the function of the base-reward, so that it could plan ahead to how the base-reward would be likely to respond to a particular set of inputs in future. 

The validity of this generalised reward-model comes down to how complicated the human base-reward actually is. Most likely, for what parts of the base-reward are clean and straightforward, it is enough for the world-model to just build up a bunch of simple predictive connections that produce good plans given the base-reward function’s output at the time. You could imagine that there is always evolutionary pressure to select these kind of clean functions when wire-heading or the complexity of the task is not an issue, because this will require dramatically less compute. As an example, the base-reward for very specific things like fear of spider-like objects ([if there even is such a base-reward function][2]) can be fairly straightforward: there is no necessary anti-wire-heading pressure, partly because the nature of the task of avoiding spiders is not particularly complex. 

But where wire-heading is a sufficiently big issue, or where the task is very complex, then the base-reward function is going to have to be hard to learn and therefore very messy, and a generalisable reward-model is going to be necessary for high performance on certain tasks. The function within the base-reward that deals with sex is one of the most obvious candidates for this. There is huge selective pressure to perform well at all times, huge pressure on the function to avoid wire-heading, and huge pressure for complex, long-term planning, given all the social cognition that goes into the task. The human agent must be able to reason that more sex on the long-term is always good (regardless of immediate satisfaction),and also be able to balance this out with the millions of complex scenarios where suppression of the immediate sexual drive will save the agent from getting killed. 

  




[1]:	https://www.lesswrong.com/posts/yDRX2fdkm3HqfTpav/approving-reinforces-low-effort-behaviors
[2]:	https://www.alignmentforum.org/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome?commentId=8Fry62GiBnRYPnpNn