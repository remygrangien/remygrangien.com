---
layout: post
title: Cognition as Self-Modifiability 
---

1  

The human brain may have low modifiability in the grand scheme of things, and so self-modification in humans often takes place externally. For instance, consider the task of recollecting a number. If the number is a single digit, we can typically trust our memory. If it is four digits, we may consider writing it down, depending on how important the number seemed to us (e.g., a credit card pin typically stays in memory better than a street number we only plan on recalling once). As the number grows in digits, we become surer that, for the sake of future recollection, we had better write the number down.  

A two-part process is occurring here. The first part is an ability to informally self-model, i.e. model the way in which our own mind works, such that we can reasonably work out whether we will remember the number or not. The second part is self-modification, i.e. the ability to modify the current or future state of our mind, such that we can improve the chance that we recall it. Because we would like to ensure the recollection of the number, we use our self-model to choose the appropriate method of modification. Often repeating a number out-loud multiple times or stressing its importance consciously will aid in recollection later. At times, we know assuredly that we our incapable of memorizing a number, and so we modify something like a piece of paper, so that in future we can access the information (in a sense, the modification of the paper is a modification of our future brain-state). Whether or not the piece of paper therefore constitutes the ‘self’ or ‘mind’ is unimportant – what is important is that the purpose of writing the number down is that we successfully recall it later on. That humans have to rely on external tools in this way suggests that we have a decent enough ability to self-model, but perhaps not so great of an ability to self-modify: we cannot reach right into our brain and bake in an arbitrarily long number, so that it will be reliably there for recollection.

2 

This capacity to self-modify is a component or measure of intelligence that sets humans apart from other mammals. Clearly it is only a small component of our overall intelligence, with the greater measure being best defined as a capacity to learn. I would like to consider where this capacity to learn gives rise to a capacity to self-modify.  

I propose that the capacity to learn is a straight-forward measure to make. Simply, the capacity to learn reduces directly from a capacity to perform. For example, given an activity like chess, the capacity to learn chess equals how well you can play the game. In this sense, Alpha Zero’s capacity to learn chess is greater than any human grandmaster’s (whether or not this capacity to learn is more efficient is another matter, and is not included in the measure as it pertains to intelligence). Of course, what does matter when it comes to an overall measure of the intelligence of an agent is that Alpha Zero’s capacity to learn generally is poorer than the grandmaster’s. While Alpha Zero greatly overpowers the grandmaster at the game, the artificial agent cannot teach chess, recall the history of chess, organize its own learning schedule, drive a car, etc. Hence we typically take Alpha Zero to be less generally intelligent than a human (or simply less intelligent, depending on how instrumental to power we take generality to be). 

The nature of a general measure of the capacity to learn is debatable. For some it is simply some kind of near-linear measure of the learning capacity over some set of tasks. For others, generality is not a linear or quantitative measure, but rather some threshold beyond which anything can be learned. As per this latter definition, humans are generally intelligent not because we can learn any task (we presently cannot), but that it is possible that we could. For example, the capacity to learn calculus is close to zero if we have not yet learned how to speak a language or perform arithmetic, however it is clear that we could be taught these things without major changes to the learning algorithm of our brain. Even when there are some tasks that we will never be able to perform in almost any capacity due to limitations in the computational capacity of the brain, we possess a general ability to reason that allows us to formalize these deficiencies and so find external methods of completing the tasks. Important to note about this qualitative measure is that it implies that other mammals are equally generally intelligent when compared to humans, and it is only computational capacity (due most likely to a scaling of the neocortex and subtle changes in its interface with areas like the thalamus), that sets us apart in what we have managed to learn. For instance, other mammals can generally learn language, but the amount of language that they can fit in is significantly lower, and from this sort of discrepancy all of our behavioral differences arise.

3 

These measures of intelligence give us a rather nice lens with which to view the evolution of biological intelligence, as well as hopefully generate useful intuitions for the space of the forms artificial intelligence may take.  

At the beginning of intelligence, we get task-specific learning algorithms, as found in animals like worms and reptiles (some might even like to quantify certain capacities of single cells in this manner). At some point, there appears something like a common-cortical algorithm that can generally model the world, and so be used to better plan for rewards, thereby flexibly performing tasks. This is the sort of learning apparatus which we have observed to readily take over input from different sensory modalities and yet still successfully model the world. Finally, generality is closely followed by the capacity to self-modify: the model of the world extends to the mind itself, and so the agent can plan its behavior around the common-cortical algorithm and other structures of the brain, so as to maximize its performance, modify its goals, or otherwise make up for its structural deficiencies.  

As we have already seen, the capacity to self-modify can be broken down into two distinct sub-measures: the quality of the self-model, and the actual degree of modification that can be performed on the agent. Neither of these sub-measures seem to have advanced much from primates to humans, and so it is tempting to suggest that these measures are really just an unavoidable feature of generality (specifically, self-modifiability equals the combination of generality, some threshold of computational capacity and the right learning material). But while it might be correct to say that self-modification cannot occur without generality (for how can an agent model itself without a capacity to model the world?), I think it is incorrect to say that generality must include the ability to self-model or modify. Moreover, this is a dangerous thing to assume from an existential risk perspective. It will be perfectly possible to create artificial agents with high-generality, yet that are incapable of modelling or modifying themselves (in fact this is an explicit goal of many AI safety approaches), and this will dramatically change the potential behavior of that agent.  



