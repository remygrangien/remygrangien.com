---
layout: post
title: "The world-model as an agent"
---

This post aims to capture a first approximation of the intuition that the ‘world-model’ of a generally intelligent agent has agential properties devoid of its reward-model. This runs counter to a large swathe of intuitions in machine learning and neuroscience. If taken to be true, this intuition produces predictions about agent behaviour that differs significantly from those that typically follow from the orthogonality and instrumental convergence theses.

This line of thinking can be traced back most obviously to Aristotle, who identified the rational part of a human as being a distinct, active capacity. There were plenty of good reasons to doubt this, and it is in Hume that we find a rather final distinction between the intellect and the passions, which has held for most philosophies of mind since. Hume’s claim is that if the intellect were to be separated from the passions, it is rendered catatonic. If we look to contemporary reenforcement learning agents, this notion will be readily confirmed. Even for a theorist like Jeff Hawkins, who is attempting to work backward from the general intelligence found in the human brain, it is assumed that the world-model is like a map that has no capacity to act on the world beyond its reward-model.

To build intuitions that counter this Humean claim, it is worthwhile to briefly define the notions ‘agent’, ‘world-model’, ‘reward-model’, etc., because each concept readily goes under different names in different circles.

1\. Terminology

I take the cleanest first approximation of what agent means to be that which acts coherently under a utility function. It is a clean approximation conceptually, but in practice leaves plenty unclear. Is the human best understand as a single agent? Often the assumption is yes, meaning that however messy our set of rewards are, it is still perfectly reasonable to assume that they could be mapped out as a utility function, without this glossing over the complexity of the interaction of that utility function with itself.

I think this assumption is prone to lead to conceptual failures down the track (particularly the Humean one), so I defer to the term ‘reward-model’ instead. It is clear that while the rewards a human is instantiated with genetically are coherent to the extent that by-and-large the body they reside in stays intact, it is not clear at all that they are coherent in the sense that they are essentially part of one larger reward. In order for the human to function without perfect rational oversight, the rewards have to be messy and pull the world-model and each other in directions that are rather incoherent.

The key point to make here is that the reward-model as a whole — which is what we would also call the utility function of the agent — is self-reflexive and modifies itself all the time. It is possible for some rewards to win over and dominate others, such that individual reward-signal themselves are altered by another into the future, e.g. it is possible for the sex drive to get out of hand in such a way that it permanently alters rewards associated with other regulative functions like sleep.

This leads us to the problem of utility functions nested inside of utility functions. Unless we would like to restrict our notion of an agent to the one utility function which controls for the agent’s output in a given moment, it is equally as sensible to state that the human brain is comprised of multiple utility functions, i.e. of multiple agents. Again, this makes no difference to what is going on on the ground, but it is a handy distinction to make if you do not want to preemptively rule out the intuition that the world-model may have agential proprieties.

2\. Learning algorithms as agents

A view consistent with most of the literature in machine learning is that the world-model is either built by or identical with a set of learning algorithms. Increasingly the view is that the set of learning algorithms taken to be necessary for general intelligence are simple and uniform, and that much of the gap between mammals or current ML agents when compared to general intelligence is accountable to scale.

The world-model under this view is instantiated as empty, except for these simple and largely uniform learning algorithms. Sensory input comes in, and guided by the reward-model, structure builds up in the world-model that allows for increasingly complicated behaviour. Typically the story stops there: all the structure of general intelligence is reducible to the guidance of the reward-model, which is seperate from the world-model, and all behaviour outputted by the agent is reducible to that reward-model eliciting behavioural structure contained in the world-model. If you unplug the reward-model early on, the world-model structure would not have developed, and if you unplug it later on, the world-model structure will not be used. Either way, the world-model separated from the reward-model is catatonic.

But a learning algorithm without a supervising signal doesn’t just do nothing. It still takes in inputs and transforms them into outputs based on existing weights and hyper-parameters. There is nothing stopping these outputs from being motor outputs, which then of course changes the inputs. While it is correct to say that at almost all low levels of intelligence (e.g. structure in the world-model), the motor outputs largely look meaningless, it is not immediately clear why they could become less meaningless as complexity scales. This is precisely the argument that I would like to make: as you approach general intelligence, or at least whatever it is humans typically achieve within a few years of life, you get coherent, long-term plans that stem almost solely from whatever it is that allows the world-model itself to learn.

3\. Examples of agent outputs that seem attributable to the world-model

Infants will regularly turn their hands over as they study them visually, and will do the same when they first grasp objects. It is obvious that their visual cortex, and more generally their neocortex, is doing this to build a better model of their hand and the particular object. I cannot think of a reward that we would typically assign to the reward-model that this repeated action can be attributable to: the hypothesis here is that this action you see across almost all infants is being performed entirely by the general learning algorithm that is responsible for learning a spatial model of the body and world.

This sort of infant behaviour can be contrasted with more complex ones like crying to optimise for the obtaining of something desirable like a particular good. And will it is clear that the hand-twirling is rather simple in comparison, the fundamental principle is made: the world-model requests plenty of outputs for the sake of optimising for something like predictability of the world.

As an infant becomes a child, and furthermore when becoming an adult, the reward-model ramps up massively and often does completely dominate the output. In fact, it seems clear why the reward-model must stay so muted throughout infancy: the longer it stays out of the way of the world-model, the more complex that world-model will become.

In adults, I’d argue that there’s a good chance that the tendency to contemplate when the reward-model goes quiet is a demonstration of the residual activity of the world-model taking place. Meditation, psychedelics and traditionally religious experiences are all associated with a the temporary quieting of the reward-model, and from there the strengthening of the proclivity to be motivated entirely by ‘intellectual pursuits’, e.g. philosophy, the sciences, the arts, etc. I would hypothesise here that the learning algorithm that asks the infant to turn over their hands so the eyes can study them is the same learning algorithm that is asking the purely contemplative scientist to form a plan on how to optimise for the predictability of something like the future observations of planets.

An obvious objection, then, is why the predictability of something like the future observations of planets, and not any other arbitrary part of the world? Even if the learning algorithm will spur on behaviour to minimise uncertainty of anything, the object that it focuses on seems like it should be ultimately attributable to some already instated reward. I do think this objection holds well if we only take rather regular cases of behaviour as an example, e.g. the scientist that is motivated to reduce the uncertainty of the moments of the planet ultimately being in science because he’s trying to reduce anticipated pain, which means so and so, and requires this and that pay check, etc.

Consider instead the autistic mind that hopelessly gets lost down rabbit-holes of learning at the expense of almost all other rewards, e.g. going for hours if not days at a time with minimal food, sleep, social interaction, etc. I would argue here that the autistic mind is an edge case of the human phenotype, which gets the learning algorithms wrong in a way that would often lead to demise in the ancestral environment. Specifically, the learning algorithms of the world-model are too strong, and are not informed sufficiently by the supervisory signal of the reward-model, leading to a structure of world-model poor relative to selective pressures. It is not that the usual reward-model was not there to begin with, rather it is that it is being dominated by the learning algorithm. If it seems like the usual reward-model is not there, it is because it is merely undeveloped due to the overwhelming focus on what the world-model is after.

4\. Characterising the utility function of the world-model

What characterises a learning algorithm is quite unsurprisingly learning. For the sake of this argument, I will take it that to learn is essentially SGD, or more broadly the optimisation of predictability for some target function. At low-levels of scale, a learning algorithm cannot elicit meaningful or complex behaviour unless there is some supervisory signal, i.e. a target function, which points to the part of the world to predict for. But as the world-model is scaled instrumentally for the reasons of optimising for the reward-model, is will continue to have opportunities to optimise itself further when the reward-model largely goes silent. Because it is already in the habit of making long-term, coherent plans for the pursuit of simple rewards, it has the tools necessary to form these same plans for the sake of the reward the learning algorithm is looking for. In this sense, the world-model can be considered a kind of mesa-optimiser for the base-optimiser which is the reward-model. But rather than the usual characterisation of the mesa-optimiser merely being an aligned version of the reward-model, what makes it unaligned is due to something that it was instated with, e.g. those things required to make a learning algorithm function at all.

Plenty, if not the supermajority, of historical and contemporary philosophy can be attributable to an attempt to maximise for the instantiated reward-function, i.e. to minimise for suffering. However, I do think it is plausible that some amount of philosophical reflection, especially meta-ethical reflection, is stemming from the reward the world-model is looking for, that being the better situating of itself in the world, or the minimisation of uncertainty of future observations.

5\. Conclusion

It is for these reasons that I think a case can be made for the world-model of the human brain being an agent in of itself, one that is ready to be at odds with other rewards present in the brain. The key claim here is a rejection of Hume’s: the intellect, if separated from the passions, is not totally catatonic. While it is the case that for almost all intelligences other than humans, an agent without a reward-model would be rendered catatonic, the human mind on the other hand has enough structure to at least act a little coherently. Whether or not any ethical actions would result is a far harder question, and will depend on which meta-ethical position you take up. However, I hope that it is clear that plenty of human behaviour seems non-ethical in the Humean, and is the direct result of a world-model trying to mindlessly build a better model of its surroundings.
