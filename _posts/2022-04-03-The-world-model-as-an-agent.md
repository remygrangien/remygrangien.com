---
layout: post
title: "The world-model as an agent"
---

This post aims to capture the intuition that the world-model of a general intelligence is an agent of its own. This intuition runs counter to a large swathe of thinking in contemporary machine learning and neuroscience. If taken to be correct, this intuition produces predictions about agent behaviour that differs to those that typically follow from the orthogonality and instrumental convergence theses. 

This line of thinking can be traced back most obviously to Aristotle, who identified the rational part of a human as being a distinct, active capacity. There were plenty of good reasons to doubt this, and it is in Hume that we find a rather final distinction between the intellect and the passions, which has held for most theories of mind since. Hume’s claim is that if the intellect were to be separated from the passions it would be rendered catatonic. When we look to most contemporary reenforcement learning agents, they seem to conform to this principle. Even a theorist like Jeff Hawkins, who is working backwards from the general intelligence found in the human brain, the assumption is that the world-model is akin to map, possessing no capacity to act in the world beyond whatever the reward-model asks of it.

So as to counter this Humean claim, it is worthwhile to briefly define the notions ‘agent’, ‘world-model’, ‘reward-model’, etc., because each concept readily goes under different names in different circles. 

### 1. Terminology

As a first approximation, an agent is that which can be described as acting coherently in its environment due the maximisation of some utility function. It is a clean approximation conceptually, but in practice leaves plenty unclear. Is the human best understood as a single agent? The typical assumption is yes, meaning that however messy our set of rewards are, it is reasonable to assume that they could be mapped out as a single utility function, without this glossing over the complexity of the interaction of that utility function with itself.

I think this assumption is prone to conceptual failures down the line, so I defer to the term ‘reward-model’ instead. While the different reward signals a human is genetically instantiated with are coherent to the extent that the body they reside in stays intact (most of the time), it is not clear that they are coherent in the sense that they do not chaotically attack one another. In order for the human to function without decent rational oversight, the rewards have to be messy and pull the world-model and each other in directions that are rather incoherent. 

While it seems rather arbitrary playing around with the definition of coherency like this, the point worth observing is that the terming of the human reward-model as ‘a single utility function’ tends to cause us to gloss over how this utility function is self-reflexive and modifying of itself continually. It is possible for some rewards to win over and dominate others, such that individual reward-signals are altered by others into the future, e.g. it is possible for the sex drive to get out of hand in such a way that it permanently alters rewards associated with other regulative functions like sleep down the line. 

Unless we would like to restrict our notion of an agent to the one utility function which controls for the agent’s output in a given moment, it is equally as sensible to state that the human brain is comprised of multiple utility functions, i.e. of multiple agents interacting with each other. Again, this difference in definition does not change what is going on on the ground, but it is a handy distinction to make if you would like to avoid preemptively ruling out the intuition that the world-model may have agential proprieties. 

### 2. Learning algorithms as agents

A view consistent with most of the literature in machine learning is that the world-model is built by a set of learning algorithms. Increasingly the view is that this set of learning algorithms are simple and uniform, and that much of the gap between mammals (or current RL agents) and general intelligence is due to scale. 

The world-model under this view is instantiated as empty, except for these simple and largely uniform learning algorithms. Sensory input comes in, and guided by the reward-model, structure builds up in the world-model that allows for increasingly complicated behaviour. Typically the story stops there: all the structure of general intelligence is reducible to the guidance of the reward-model, which is seperate from the world-model, and all behaviour outputted by the agent is reducible to that reward-model eliciting behavioural structure contained in the world-model. If you unplug the reward-model early on, the world-model would not have developed. If you unplug it later on, the world-model will not be used. Either way, the world-model separated from the reward-model is catatonic. 

But not all learning algorithms without supervising signals do nothing. Some still take in inputs and transform them into outputs based on existing weights. They also update those weights based on certain sub-rewards and hyper-parameters (these sub-rewards are not those relating to the reward-model, but really basic things like pattern recognition). There is nothing stopping these outputs from being motor outputs, which then of course change inputs. While it is correct to say that at almost all low levels of intelligence (e.g. structure in the world-model), the motor outputs determined by the world-model for this learning purpose look largely meaningless, it is not immediately clear why they could not become less meaningless as complexity scales. And so the argument I would like to consider is whether as you approach general intelligence, or at least whatever intelligence it is that humans typically achieve within a few years of life, you can get some coherent, long-term plans that stem solely from whatever it is that enables the world-model to learn. 

### 3. Examples of agent outputs that seem attributable to the world-model alone

Infants will regularly turn their hands over as they study them visually, and will do the same when they first grab objects. It appears that this is an instrumental motor output devised by the world-model for the purpose of building a better model of the infant’s hand and the particular object. I cannot think of a reward that we would typically assign to the reward-model that this repeated action can be attributable to. The hypothesis then is that this action observed in almost all infants is being performed entirely by the learning algorithm that is responsible for learning a spatial model of the body and world. 

This sort of infant behaviour can be contrasted with more complex ones like crying to optimise for the obtaining of something desirable like a particular food. And while it is clear that the hand-rotating is a rather simple motor output in comparison to this policy for food acquisition, the fundamental principle is argued: the world-model requests plenty of outputs for the sake of optimising for something like predictability of the world. 

As an infant becomes a child, and furthermore when becoming an adult, the reward-model ramps up massively and often completely dominates output. It would seem that it is for this reason that the reward-model stays so muted throughout infancy: the longer it stays out of the way of the world-model, the more sophisticated that world-model can become.

In adults, I’d argue that there’s a good chance that the tendency to contemplate when the reward-model goes quiet is a demonstration of the residual activity of the world-model. Meditation, psychedelics, and certain religious experiences are all associated with the temporary quietening of the reward-model, and from there the strengthening of the proclivity to be motivated entirely by ‘intellectual pursuits’, e.g. philosophy, the sciences, the arts, etc. I would hypothesise here that the learning algorithm that asks the infant to turn over their hands so the eyes can study them is the same learning algorithm that is asking the purely contemplative scientist to form a plan on how to optimise for the predictability of something like the future observations of planets. 

An obvious objection is to ask what it is that singles out the predictability of something like the future observations of planets, and not any other arbitrary part of the world? Learning a model of your hands seems pretty instrumental, especially in light of how well it will help an agent to maximise a large number of diverse rewards, whereas ‘where can I expect the planets to be’ does not seem so instrumental and relevant. Even if the learning algorithm will spur on behaviour to minimise uncertainty of practically anything, the object that it focuses on seems ultimately selected by some already instantiated reward-signal (or sub-reward of nth degree). For instance, the plan to reduce the uncertainty of the moments of the planet was selected to reduce for the uncertainty of future pay-checks, etc. I agree that the supermajority of complex human contemplative behaviour is attributable in this way to the reward-model, but there are some promising edge cases. 

Consider the autistic mind that gets hopelessly lost down rabbit-holes of learning at the expense of almost all other rewards, e.g. going for days at a time with minimal food, sleep, social interaction, etc. I would argue here that the autistic mind is an edge case of the human phenotype, which gets the learning algorithms wrong in a way that would often lead to demise in the ancestral environment. Specifically, the drive of the learning algorithms are too strong, and so not informed sufficiently by the supervisory signal of the reward-model, leading to a structure of world-model poor relative to selective pressures. It is not that the usual reward-model was not there to begin with, rather it has always been dominated by the learning algorithm. If it seems like the reward-model is not fully there, it is because it is merely undeveloped due to the overwhelming focus on what the world-model is after (this is not to say that a subdued instantiated reward-model would not also lend itself to this behaviour). 

### 4. Characterising the utility function of the world-model

Unsurprisingly, what characterises a learning algorithm is its desire to learn. For the sake of the argument, I take it that to learn is essentially SGD, or more broadly the optimisation of predictability for some input. At low-levels of scale, a learning algorithm cannot elicit meaningful or complex behaviour unless there is some supervisory signal which points to the part of the world to predict for. But as the world-model is scaled instrumentally for the reasons of optimising for the reward-model, these instruments remain available for its own use as well. Because it is already in the habit of making long-term, coherent plans for the pursuit of simple rewards, it has the tools necessary to form these same plans for the sake of the reward it is itself looking for. In this sense, the world-model can be considered a kind of mesa-optimiser for the base-optimiser which is the reward-model. But rather than the usual mesa-optimiser, characterised by being an unaligned instantiation of the base-reward, what makes the learning algorithm unaligned is its drive to learn, which can get out of hand at certain scales of intelligence. 

Plenty, if not the supermajority, of historical and contemporary philosophy can be attributable to the maximisation of the instantiated reward-model, i.e. to minimise for suffering. However, I do think it is plausible that some amount of philosophical reflection, especially meta-ethical reflection, is stemming from the reward the world-model is looking for, that being the minimisation of uncertainty over future observations of things like itself and all other agents. 

### 5. Conclusion

I think a case can be made that the world-model of the human brain exhibits agential behaviour. Here, ‘agential behaviours’ means that the world-model is driven to formulate complex outputs to capture of reward that it itself possess, rather than some reward coming from outside the world-model. These rewards and outputs can be at odds with the reward-model present in the largely non-neocortex parts of the brain. 

The key claim here is the antithesis of Hume’s: the intellect, if separated from the passions, is not necessarily cationic. While it is the case that for almost all intelligences other than humans, an agent without reward-model would be rendered catatonic, the human mind on the other hand has enough structure to at least act a little coherently. This is to say that the neocortex’s drive to learn would continue to generate outputs that would be fairly coherent. Whether or not any ‘ethical actions’ would result is a far harder question, and will depend on what meta-ethical position you assume. However, I hope that it is clear that plenty of complex human behaviour that seems non-ethical in the Humean sense is the direct result of a world-model trying to mindlessly build a better picture of its surroundings. 