---
layout: post
title: Blackbirds and pigeons
---

In *Philosophy of Mind: Body, Brian and Behaviour* by Slor et al. (2015), I came across a thought experiment of interest, which I paraphrase here: 

>It is a foggy morning. As you look outside your window, you see a bird sitting on a branch of a tree. You mistake the bird for a pigeon, when it is in fact a blackbird. 

Of the comments made by the authors, this struck me as particularly misleading: 

>“Once a token of PIGEON in your head has been caused by a blackbird, tokens of the PIGEON type obtain the meaning ‘pigeon-or-blackbird’. The undesirable consequence is that you are not making a mistake when you instantiate a token of PIGEON in response to seeing a blackbird.” (Slor et al. pp.127)	

This is taken to be an argument against resemblance theories of intentional content. While resemblance theories of intentional content is not something I have fully related to my own understanding of the mind, this argument against it does seem relatable, and I believe it stems from a misunderstanding of how the brain recognises objects. 

To operate efficiently in the world, the brain must reuse features in its identification of objects. These shared features can be as fundamental as the presence of edges, but also as complex as the presence of legs, of particularly coloured or textured plumage, etc. And so the model that underlies the token PIGEON, while it is in its totality different from  the model that underlies the token BLACKBIRD, shares many of the same features, because blackbirds and pigeons share these features in the world. An attempt to engineer object recognition is not needed to demonstrate this, but merely our own robust object detection observed: a pigeon is in some ways less similar to another pigeon when compared to a blackbird with the same number of feathers (or legs, etc.), yet we can still tell that what we are looking at is a pigeon, and not a blackbird, because there is overall enough differentiating features, or combinations of certain features, presented for differentiation, and thus identification.

That our ability to classify objects requires generalised models is a widely held belief, however we must still address the worry that these models of classes are flimsy, such that our model of a blackbird may as well be a model of a pigeon when we fail to classify a blackbird correctly. 

In the particular scenario presented to us, the key to diffusing this worry is the presence of the fog. This is because, without the fog or any other obscurant, if we still misidentify the bird, we must ask ourselves whether we do indeed have different models for blackbirds and pigeons. If no amount of clarity of features can produce a definitive decision, then it’s because those features have not been well associated with the relevant token. 

This intuition can be demonstrated in reverse, and provides us with a different and useful premise. Say you know nothing about either blackbirds and pigeons, other than that they are types of birds. Your friend that does know the difference would like to help you learn, and has prepared for you a deck of labelled photographs of these birds. Unfortunately, your friend is not the most wise teacher: each bird he has taken a photograph of is heavily obscured by fog, such that no definitive features of the birds can be readily extracted from the images (they are however photographs of real pigeons and blackbirds, and he has labelled them correctly based on further observations he has made after taking them). Furthermore, what little features can be identified are not so useful, because the number of photographs he has provided is so small when compared to their quality. 

From this example, I hope the intuition is that you could not learn the respective models for blackbird and pigeons from this selection of photographs, despite their being labelled examples. We may now suggest that, in the case where misidentification occurs: 

1. It occurred because the features presented were not clear. 
2. In the case where the features presented were not clear, the underlying model for the incorrect token was not significantly updated, i.e. nothing was learned. 
